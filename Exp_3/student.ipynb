{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习 实验3\n",
    "\n",
    "本节课我们实现包含隐层的神经网络模型，并在手写数字识别数据集上进行测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集\n",
    "\n",
    "手写数字识别数据集\n",
    "\n",
    "下载链接：https://www.kaggle.com/c/digit-recognizer/data\n",
    "\n",
    "（本节课仅需要训练集）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码说明\n",
    "\n",
    "下面的代码构造了一个包含隐层的神经网络模型，模型分为输入层、隐含层、输出层\n",
    "\n",
    "输入层包含 $784$ 个神经元，输入手写数字的灰度向量，输出 $U$\n",
    "$$\n",
    "U=X\n",
    "$$\n",
    "\n",
    "隐含层包含 $30$ 个神经元，输入 $U$，输出 $V$\n",
    "$$\n",
    "V=\\sigma(W_1U)\n",
    "$$\n",
    "\n",
    "输出层包含 $10$ 个神经元，输入 $V$，输出 $\\hat Y$ 表示每个数字的概率\n",
    "$$\n",
    "\\hat Y=\\sigma(W_2V)\n",
    "$$\n",
    "\n",
    "其中 $\\sigma$ 表示 Sigmoid 函数，每个样本数据都以列向量进行存储和计算，损失函数取所有样本预测值的均方误差\n",
    "\n",
    "$$\n",
    "Loss=\\frac{1}{N}\\sum_{i=1}^N (\\hat Y_i-Y_i)^2\n",
    "$$\n",
    "\n",
    "但是，这个模型的效率太低了，请做出适当修改（**或删了重写**），以提高训练效率和准确率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可选的修改方式\n",
    "\n",
    "### 1. 向模型中添加阈值\n",
    "\n",
    "$$\n",
    "U=X+B_0\n",
    "$$\n",
    "$$\n",
    "V=\\sigma(W_1U+B_1)\n",
    "$$\n",
    "$$\n",
    "\\hat Y=\\sigma(W_2V+B_2)\n",
    "$$\n",
    "\n",
    "（在这个模型中加不加阈值影响不大。。。）\n",
    "### 2. 改为随机梯度下降\n",
    "\n",
    "代码中每次迭代会计算所有样本的均方误差，这是极其低效的，可以修改为每次取一批样本（例如128个）计算损失和梯度并更新参数。\n",
    "\n",
    "### 3. 修改激活函数与损失函数\n",
    "\n",
    "手写数字识别分类问题本质是多分类问题，对于多分类问题，一个常用的组合是 Softmax 函数与交叉熵损失，这可以提高模型的收敛速度。\n",
    "\n",
    "### 4. 使用高效的优化算法\n",
    "\n",
    "随机梯度下降法固然是一个高效的优化算法，但也可选择更高效的优化算法，例如 Adam 算法。\n",
    "\n",
    "### 5. 调参\n",
    "\n",
    "A：这就是你的深度学习系统？\n",
    "\n",
    "B：是呀！你把数据倒进这一大堆线性代数里，再到另一边等着答案出来。\n",
    "\n",
    "A：如果答案是错的呢？\n",
    "\n",
    "B：那就拿根棍子搅一搅这堆东西，一直搅到答案正确未为止。\n",
    "\n",
    "![](https://i.loli.net/2021/03/29/dTsolpvPxk92YXe.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 从 DataFrame 中读取x与y，转化为 numpy 数组\n",
    "def get_x_y(data):\n",
    "    x = data[[i for i in list(data.columns) if i != \"label\"]].values\n",
    "    # 将标签转化为 one-hot 向量\n",
    "    y_list = []\n",
    "    for i in data[\"label\"]:\n",
    "        y = [0] * 10\n",
    "        y[i] = 1\n",
    "        y_list.append(y)\n",
    "    return np.array(x) / 255, np.array(y_list)\n",
    "\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "train_x, train_y = get_x_y(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.078\n",
      "0.741\n",
      "0.814\n",
      "0.877\n",
      "0.894\n",
      "0.896\n",
      "0.903\n",
      "0.925\n",
      "0.923\n",
      "0.922\n",
      "0.931\n",
      "0.944\n",
      "0.93\n",
      "0.958\n",
      "0.934\n",
      "0.938\n",
      "0.939\n",
      "0.934\n",
      "0.938\n",
      "0.955\n"
     ]
    }
   ],
   "source": [
    "# 计算准确率\n",
    "def accuracy(y_pred, y_true):\n",
    "    ac = 0\n",
    "    for i in range(y_true.shape[1]):\n",
    "        # 计算预测的标签\n",
    "        temp = [y_pred[j][i] for j in range(10)]\n",
    "        label_pred = temp.index(max(temp))\n",
    "        # 计算真实的标签\n",
    "        temp = [y_true[j][i] for j in range(10)]\n",
    "        label_true = temp.index(max(temp))\n",
    "        if label_pred == label_true:\n",
    "            ac += 1\n",
    "        # print(label_pred,label_true,temp)\n",
    "    return ac / y_true.shape[1]\n",
    "\n",
    "\n",
    "# 神经网络的层（啥都不做的层）\n",
    "class DenseLayer:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "\n",
    "    def connect(self, layer):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "    def backward(self, dy):\n",
    "        return dy\n",
    "\n",
    "\n",
    "# 神经网络的层（全连接层）\n",
    "class LinearLayer:\n",
    "    # 初始化\n",
    "    def __init__(self, n, activation, activation_diff):\n",
    "        # 神经元个数\n",
    "        self.n = n\n",
    "        # 激活函数\n",
    "        self.activation = activation\n",
    "        # 激活函数导数\n",
    "        self.activation_diff = activation_diff\n",
    "        # 学习率\n",
    "        self.eta = 0.001\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.iter = 0\n",
    "        self.mw = None\n",
    "        self.vw = None\n",
    "        self.mb = None\n",
    "        self.vb = None\n",
    "\n",
    "    # 全连接，随机初始化 w\n",
    "    def connect(self, layer):\n",
    "        self.w = np.random.rand(self.n, layer.n) * 0.2 - 0.1\n",
    "        self.b = np.zeros((self.n, 1))\n",
    "        self.mw = np.zeros_like(self.w)\n",
    "        self.vw = np.zeros_like(self.w)\n",
    "        self.mb = np.zeros_like(self.b)\n",
    "        self.vb = np.zeros_like(self.b)\n",
    "\n",
    "    # 前向传播\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.u = np.matmul(self.w, self.x) + self.b\n",
    "        self.y = self.activation(self.u)\n",
    "        return self.y\n",
    "\n",
    "    # 反向传播 传入损失函数对该层每个输出值的导数，传出损失函数对该层每个输入值的导数\n",
    "    def backward(self, dy, update=True):\n",
    "        self.du = dy * self.activation_diff(self.u)\n",
    "        self.db = self.du.mean(axis=1)\n",
    "        self.dw = np.matmul(self.du, self.x.T)\n",
    "        self.dx = np.matmul(self.w.T, self.du)\n",
    "        if update:\n",
    "            self.iter += 1\n",
    "            self.mw = self.beta1 * self.mw + (1 - self.beta1) * self.dw\n",
    "            self.vw = self.beta2 * self.vw + (1 - self.beta2) * (self.dw ** 2)\n",
    "            self.mw_hat = self.mw / (1 - (self.beta1 ** self.iter))\n",
    "            self.vw_hat = self.vw / (1 - (self.beta2 ** self.iter))\n",
    "            self.vw_sqrt = np.sqrt(self.vw_hat)\n",
    "            self.w -= self.eta * self.mw_hat / (self.vw_sqrt + 0.00000001)\n",
    "            self.mb = self.beta1 * self.mb + ((1 - self.beta1) * self.db).reshape(self.n, 1)\n",
    "            self.vb = self.beta2 * self.vb + ((1 - self.beta2) * (self.db ** 2)).reshape(self.n, 1)\n",
    "            self.mb_hat = self.mb / (1 - (self.beta1 ** self.iter))\n",
    "            self.vb_hat = self.vb / (1 - (self.beta2 ** self.iter))\n",
    "            self.vb_sqrt = np.sqrt(self.vb_hat)\n",
    "            self.b -= self.eta * self.mb_hat / (self.vb_sqrt + 0.00000001)\n",
    "        return self.dx\n",
    "\n",
    "\n",
    "# sigmoid 激活函数 f(x)=1/(1+exp(-x))\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# sigmoid 激活函数的导数 f'(x)=f(x)(1-f(x))\n",
    "def sigmoid_diff(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "# softmax 函数\n",
    "def softmax(x):\n",
    "    shift_x = x - np.max(x)\n",
    "    exp_x = np.exp(shift_x)\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "\n",
    "# 无效激活函数\n",
    "def invalid(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def invalid_diff(x):\n",
    "    return 1\n",
    "\n",
    "\n",
    "# 随机抽取指定数量的数据并转置\n",
    "def random_choice(n, x, y):\n",
    "    random_int = np.random.randint(0, len(x), n)\n",
    "    mini_x = np.zeros((n, len(train_x[0])))\n",
    "    mini_y = np.zeros((n, len(train_y[0])))\n",
    "    j = 0\n",
    "    for i in random_int:\n",
    "        mini_x[j] = train_x[i]\n",
    "        mini_y[j] = train_y[i]\n",
    "        j += 1\n",
    "    return mini_x.T, mini_y.T\n",
    "\n",
    "\n",
    "# 构造神经网络\n",
    "input_layer = DenseLayer(784)\n",
    "hidden_layer = LinearLayer(30, sigmoid, sigmoid_diff)\n",
    "hidden_layer.connect(input_layer)\n",
    "output_layer = LinearLayer(10, invalid, invalid_diff)\n",
    "output_layer.connect(hidden_layer)\n",
    "n = 1000\n",
    "\n",
    "for iteration in range(5000):\n",
    "    # 正向传播\n",
    "    mini_x, mini_y = random_choice(n, train_x, train_y)\n",
    "    u = input_layer.forward(mini_x)\n",
    "#     mini_x = train_x.T\n",
    "#     mini_y = train_y.T\n",
    "    v = hidden_layer.forward(u)\n",
    "    y_hat = output_layer.forward(v)\n",
    "    for i in range(n):\n",
    "        y_hat[:, i] = softmax(y_hat[:, i])\n",
    "    # 计算损失函数对每个输出值的导数\n",
    "    # d_loss = (y_hat - mini_y) / y_hat.shape[1]\n",
    "    d_loss_d_z = y_hat - mini_y\n",
    "    # 反向传播\n",
    "    dv = output_layer.backward(d_loss_d_z)\n",
    "    du = hidden_layer.backward(dv)\n",
    "    # 输出准确率\n",
    "    if iteration % 100 == 0:\n",
    "        print(accuracy(y_hat, mini_y))\n",
    "u = input_layer.forward(train_x.T)\n",
    "v = hidden_layer.forward(u)\n",
    "y_hat = output_layer.forward(v)\n",
    "for i in range(n):\n",
    "    y_hat[:, i] = softmax(y_hat[:, i])\n",
    "print(\"对于所有数据，准确度为{}\".format(accuracy(y_hat, train_y.T)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
